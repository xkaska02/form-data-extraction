{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPh+NNEbLXGKManYi4/da3C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xkaska02/form-data-extraction/blob/dev/finetune_lilt_on_forms_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFeBT79Tx7wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc87b50b-0192-45c9-8e7e-275e5122852d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YWX9R4myKH3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate seqeval"
      ],
      "metadata": {
        "id": "1vN0_BCeyCy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5835f3-9b3a-46fd-9bcb-9062de4b9760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jm5IkGnBE9UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6zuh-JaE2Zo",
        "outputId": "a2ac388b-ad5b-49fe-bc97-1333b31bf584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "wandb.login(key=userdata.get('WANDB_API_KEY'))\n",
        "# wandb.init(project=\"form-data-extraction\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIFM3zFiE_go",
        "outputId": "7f116793-d61e-46e0-a412-2d6d7da45dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxkaska\u001b[0m (\u001b[33mxkaska-brno-university-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LiltForTokenClassification, LiltModel\n",
        "import torch.nn as nn\n",
        "class LiltForTokenClassification2layer(LiltForTokenClassification):\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.num_labels = config.num_labels\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "    self.lilt = LiltModel(config, add_pooling_layer=False)\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(config.hidden_size, config.hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(config.hidden_dropout_prob),\n",
        "        # nn.Linear(config.hidden_size, config.hidden_size),\n",
        "        # nn.ReLU(),\n",
        "        # nn.Dropout(config.hidden_dropout_prob),\n",
        "        nn.Linear(config.hidden_size, config.num_labels)\n",
        "    )\n",
        "\n",
        "    self.post_init()"
      ],
      "metadata": {
        "id": "4kpGGseOG5ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "# dataset = load_dataset(\"json\", data_files={\"train\":\"train_split.json\", \"validation\":\"validation_split.json\", \"test\":\"test_split.json\"})\n",
        "dataset = load_dataset(\"json\", data_files={\"train\":\"train_split.json\", \"validation\":\"validation_split75.json\", \"test\":\"test_split.json\"})"
      ],
      "metadata": {
        "id": "8Z-oPII4yF86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-439ew2EhDyC",
        "outputId": "1f8e0a98-0999-47a6-b6b3-29e190f82246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ner_tags': [3, 3, 0, 0, 4, 0, 0, 4, 4, 4, 4, 4, 5, 0, 0, 0, 5, 5, 0, 0, 5, 5, 6, 0, 0, 0, 0, 7, 7, 7, 0, 0, 0, 0, 0, 0, 7, 7, 7, 8, 8, 8, 0, 0, 9, 0, 0, 0, 9, 9, 0, 0, 0, 0, 0, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 0, 0, 0, 0, 0, 0, 0], 'bboxes': [[881, 78, 1123, 158], [492, 117, 809, 246], [85, 137, 236, 184], [291, 135, 398, 181], [624, 255, 776, 316], [83, 280, 243, 327], [301, 280, 375, 327], [426, 310, 589, 391], [620, 316, 762, 391], [780, 315, 892, 391], [912, 314, 1043, 391], [1053, 316, 1141, 391], [985, 385, 1210, 469], [84, 434, 213, 482], [263, 434, 376, 481], [381, 434, 555, 481], [591, 395, 780, 469], [821, 391, 964, 469], [83, 466, 173, 501], [184, 465, 267, 501], [790, 475, 1015, 531], [696, 533, 1123, 612], [317, 614, 443, 686], [88, 652, 285, 690], [86, 688, 173, 727], [185, 688, 269, 727], [272, 688, 346, 727], [510, 760, 572, 837], [609, 760, 672, 837], [689, 760, 866, 837], [91, 797, 212, 838], [260, 797, 355, 838], [371, 797, 463, 838], [85, 831, 177, 875], [185, 833, 268, 875], [271, 831, 352, 875], [475, 834, 814, 911], [836, 834, 1125, 911], [262, 909, 456, 978], [432, 982, 493, 1057], [519, 979, 596, 1057], [615, 979, 775, 1057], [90, 1018, 211, 1065], [229, 1018, 362, 1065], [700, 1077, 805, 1124], [98, 1087, 260, 1141], [270, 1087, 421, 1142], [469, 1087, 569, 1141], [483, 1131, 819, 1198], [852, 1131, 1141, 1198], [102, 1318, 269, 1368], [278, 1320, 358, 1368], [365, 1321, 475, 1368], [94, 1384, 237, 1439], [245, 1384, 354, 1439], [406, 1360, 680, 1420], [737, 1360, 856, 1420], [920, 1361, 1054, 1420], [389, 1429, 499, 1506], [528, 1429, 724, 1507], [580, 1501, 805, 1573], [570, 1572, 716, 1641], [767, 1572, 865, 1641], [882, 1572, 1015, 1641], [1053, 1572, 1124, 1641], [97, 1603, 227, 1653], [229, 1607, 349, 1654], [356, 1606, 453, 1653], [93, 1675, 210, 1735], [215, 1678, 303, 1736], [306, 1679, 328, 1737], [343, 1679, 465, 1737]], 'words': ['Michal', 'Holli', 'Příjmení', 'jméno', 'vaj.', 'Hodnost', 'pluk', 'byv.', 'pěš.', 'pl.', 'čís.', '67', 'András', 'Datum', 'místo', 'narozeni', '1891,', 'Szt.', '(polit.', 'okres,', 'Liptó,', 'Maďarsko,', 'táž', 'Příslušnost', '(polit.', 'okres,', 'země)', '8.', '9.', '1916', 'Datum', 'místo', 'úmrtí', '(polit.', 'okres,', 'země)', 'Zarków,', 'Brody,', 'Halič', '8.', '9.', '1916', 'Datum', 'pohřbu', 'lete', 'Označení', 'hřbitova', 'místo.', 'Majdan,', 'Brody,', 'Oddělení,', 'číslo', 'hrobu', 'Opsáno', 'podle', 'matriky', 'býv', 'pěš.', '67,', 'ulož.', 'Košice', 'tom', 'III.', 'fol', '15', 'Úmrtní', 'kniha:', 'tom.,', 'TIskárna', 'MNO.', '-', '1163-36.'], 'id': 57, 'image_name': 'data_files/forms/images/2042098e-de1b-4367-a60e-e793a663d806.jpg'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = [\"O\",\"B-key\",\"B-information\",\"B-name\",\"B-rank\",\"B-birth_date\",\"B-nationality\",\"B-death_date\",\"B-funeral_date\",\"B-grave_location\",\"B-grave_id\",\"B-information_source\",\"B-death_book\"]\n",
        "id2label = {id:label for id,label in enumerate(label_list)}"
      ],
      "metadata": {
        "id": "NBGFU2eAzpng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "def normalize_bbox(bbox, width, height):\n",
        "  # if any(coord < 0 for coord in bbox):\n",
        "  #   raise ValueError(f\"Invalid bbox coords: {bbox}\")\n",
        "  return [\n",
        "      max(0, min(1000,int(1000 * (bbox[0] / width)))),\n",
        "      max(0, min(1000,int(1000 * (bbox[1] / height)))),\n",
        "      max(0, min(1000,int(1000 * (bbox[2] / width)))),\n",
        "      max(0, min(1000,int(1000 * (bbox[3] / height)))),\n",
        "  ]\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, dataset, tokenizer):\n",
        "    self.dataset = dataset\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # get item\n",
        "    example = self.dataset[idx]\n",
        "    # image = Image.open(example[\"image_name\"])\n",
        "    words = example[\"words\"]\n",
        "    boxes = example[\"bboxes\"]\n",
        "    ner_tags = example[\"ner_tags\"]\n",
        "\n",
        "    # prepare for the model\n",
        "    # width, height = image.size\n",
        "    width, height = (1240, 1744)\n",
        "    bbox = []\n",
        "    labels = []\n",
        "    for word, box, label in zip(words, boxes, ner_tags):\n",
        "        box = normalize_bbox(box, width, height)\n",
        "        n_word_tokens = len(tokenizer.tokenize(word))\n",
        "        bbox.extend([box] * n_word_tokens)\n",
        "        labels.extend([label] + ([-100] * (n_word_tokens - 1)))\n",
        "        # labels.extend([label] * n_word_tokens)\n",
        "\n",
        "    cls_box = sep_box = [0, 0, 0, 0]\n",
        "    bbox = [cls_box] + bbox + [sep_box]\n",
        "    labels = [-100] + labels + [-100]\n",
        "\n",
        "    encoding = self.tokenizer(\" \".join(words), truncation=True, max_length=512)\n",
        "    sequence_length = len(encoding.input_ids)\n",
        "    # truncate boxes and labels based on length of input ids\n",
        "    labels = labels[:sequence_length]\n",
        "    bbox = bbox[:sequence_length]\n",
        "\n",
        "    encoding[\"bbox\"] = bbox\n",
        "    encoding[\"labels\"] = labels\n",
        "\n",
        "    return encoding"
      ],
      "metadata": {
        "id": "nMmL6oTPz_d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  %rm -rf lilt_robeczech_lr2e-05_bs4_train5/"
      ],
      "metadata": {
        "id": "akn8qeIrhppL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name, base_model, os.environ[\"WANDB_PROJECT\"] = \"nielsr/lilt-xlm-roberta-base\",\"lilt_xlmroberta\", \"train_dataset_size_lilt\"\n",
        "# print(os.environ[\"WANDB_PROJECT\"])\n",
        "# os.environ[\"WANDB_PROJECT\"] = \"lilt_robeczech_train_size\"\n",
        "\n",
        "# model_name, base_model, os.environ[\"WANDB_PROJECT\"] = \"xkaska02/lilt-robeczech-base\", \"lilt_robeczech\", \"lilt_robeczech_train_size\"\n",
        "# model_name, base_model = \"SCUT-DLVCLab/lilt-infoxlm-base\", \"lilt_infoxlm\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "PTY3DN090KSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(dataset[\"train\"], tokenizer)\n",
        "eval_dataset = CustomDataset(dataset[\"validation\"], tokenizer)\n",
        "test_dataset = CustomDataset(dataset[\"test\"], tokenizer)"
      ],
      "metadata": {
        "id": "TXVJHG5q0PcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(test_dataset[0])"
      ],
      "metadata": {
        "id": "wGagegYV0Q0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(features):\n",
        "  \"\"\"\n",
        "  Collate function to prepare data for the model.\n",
        "\n",
        "  This function pads sequences, handles labels and bounding boxes,\n",
        "  and converts data to PyTorch tensors.\n",
        "\n",
        "  Args:\n",
        "      features (list): List of data samples.\n",
        "\n",
        "  Returns:\n",
        "      dict: Batched data ready for the model.\n",
        "  \"\"\"\n",
        "  # Extract boxes, labels, and input_ids from features\n",
        "  boxes = [feature[\"bbox\"] for feature in features]\n",
        "  labels = [feature[\"labels\"] for feature in features]\n",
        "  input_ids = [feature[\"input_ids\"] for feature in features]\n",
        "\n",
        "  # Pad sequences using the tokenizer\n",
        "  # Use return_tensors='pt' to directly get PyTorch tensors\n",
        "  batch = tokenizer.pad(\n",
        "      {\"input_ids\": input_ids},\n",
        "      padding=\"max_length\",\n",
        "      max_length=512,\n",
        "      return_tensors=\"pt\"\n",
        "  )\n",
        "\n",
        "  # Get the sequence length from the padded input_ids\n",
        "  sequence_length = batch[\"input_ids\"].shape[1]\n",
        "\n",
        "  # Pad labels and boxes to match the sequence length\n",
        "  # Ensure padding value is -100 for labels and [0, 0, 0, 0] for boxes\n",
        "  batch[\"labels\"] = torch.tensor([\n",
        "      labels_example + [-100] * (sequence_length - len(labels_example))\n",
        "      for labels_example in labels\n",
        "  ])\n",
        "  batch[\"bbox\"] = torch.tensor([\n",
        "      boxes_example + [[0, 0, 0, 0]] * (sequence_length - len(boxes_example))\n",
        "      for boxes_example in boxes\n",
        "  ])\n",
        "\n",
        "  return batch"
      ],
      "metadata": {
        "id": "SmwaZ0RnIWjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "qkD5wgcTIgFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "9u55ADElAGxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for k,v in batch.items():\n",
        "#   print(k, v.shape)"
      ],
      "metadata": {
        "id": "nwblZ-8NAMp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_model_name(base=\"lilt_robeczech\", lr=5e-5, batch_size=8, train_size=30, extra=\"\"):\n",
        "  if extra != \"\":\n",
        "    return f\"{base}_lr{lr}_bs{batch_size}_train{train_size}_{extra}\"\n",
        "  else:\n",
        "    return f\"{base}_lr{lr}_bs{batch_size}_train{train_size}\""
      ],
      "metadata": {
        "id": "otgc5UhQspml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for example in eval_dataset:\n",
        "#     assert len(example[\"input_ids\"]) == len(example[\"labels\"]), f\"Mismatch in length: {len(example['input_ids'])} != {len(example['labels'])}\""
      ],
      "metadata": {
        "id": "s1R3izvvA_6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for id, box, label in zip(batch[\"input_ids\"][0], batch[\"bbox\"][0], batch[\"labels\"][0]):\n",
        "#   if label.item() != -100:\n",
        "#     print(tokenizer.decode([id]), box, id2label[label.item()])\n",
        "#   else:\n",
        "#     print(tokenizer.decode([id]), box, label.item())"
      ],
      "metadata": {
        "id": "njoHcXwEAR3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LiltForTokenClassification\n",
        "\n",
        "model = LiltForTokenClassification.from_pretrained(model_name, id2label=id2label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWpWiomN0U72",
        "outputId": "590cd801-53fb-46ec-d3dc-ba9827d11dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LiltForTokenClassification2layer were not initialized from the model checkpoint at nielsr/lilt-xlm-roberta-base and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "metric = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "ZI6it40a0ax8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "return_entity_level_metrics = False\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    if return_entity_level_metrics:\n",
        "        # Unpack nested dictionaries\n",
        "        final_results = {}\n",
        "        for key, value in results.items():\n",
        "            if isinstance(value, dict):\n",
        "                for n, v in value.items():\n",
        "                    final_results[f\"{key}_{n}\"] = v\n",
        "            else:\n",
        "                final_results[key] = value\n",
        "        return final_results\n",
        "    else:\n",
        "        return {\n",
        "            \"precision\": results[\"overall_precision\"],\n",
        "            \"recall\": results[\"overall_recall\"],\n",
        "            \"f1\": results[\"overall_f1\"],\n",
        "            \"accuracy\": results[\"overall_accuracy\"],\n",
        "        }"
      ],
      "metadata": {
        "id": "DMC-CM0H0ck8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 2e-5\n",
        "batch_size = 4\n",
        "# base_model = \"lilt_robeczech\"\n",
        "output_name = generate_model_name(base=base_model, lr=lr, batch_size=batch_size, train_size=len(dataset[\"train\"]))"
      ],
      "metadata": {
        "id": "eZIeA43XYR2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(num_train_epochs=30,\n",
        "                                  learning_rate=1e-5,\n",
        "                                  eval_strategy=\"epoch\",\n",
        "                                  save_strategy=\"epoch\",\n",
        "                                  load_best_model_at_end=True,\n",
        "                                  metric_for_best_model=\"eval_loss\",\n",
        "                                  per_device_train_batch_size=8,\n",
        "                                  per_device_eval_batch_size=8,\n",
        "                                  run_name=output_name,\n",
        "                                  report_to=\"wandb\"\n",
        "                                  # gradient_accumulation_steps=2\n",
        "                                  )"
      ],
      "metadata": {
        "id": "Zx3Wfpgy0k9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args.output_dir = output_name"
      ],
      "metadata": {
        "id": "RvdSxuEktSXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "early_stopping = EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0001)"
      ],
      "metadata": {
        "id": "C_CL9Uzbt411"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.data.data_collator import default_data_collator\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "  def get_train_dataloader(self):\n",
        "    return train_dataloader\n",
        "\n",
        "  def get_eval_dataloader(self, eval_dataset = None):\n",
        "    return eval_dataloader\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKmxV0710sWA",
        "outputId": "f21641d0-5b1a-42fd-c32b-b9d2b382cc65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-387-2dd4568404d2>:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = CustomTrainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "vYELE3Wg0vNI",
        "outputId": "c59d32ed-9a6f-4ee0-8e19-825398fea084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250511_215424-3xs13g6u</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/xkaska-brno-university-of-technology/train_dataset_size_lilt/runs/3xs13g6u' target=\"_blank\">lilt_xlmroberta_lr2e-05_bs4_train287</a></strong> to <a href='https://wandb.ai/xkaska-brno-university-of-technology/train_dataset_size_lilt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/xkaska-brno-university-of-technology/train_dataset_size_lilt' target=\"_blank\">https://wandb.ai/xkaska-brno-university-of-technology/train_dataset_size_lilt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/xkaska-brno-university-of-technology/train_dataset_size_lilt/runs/3xs13g6u' target=\"_blank\">https://wandb.ai/xkaska-brno-university-of-technology/train_dataset_size_lilt/runs/3xs13g6u</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='145' max='4320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 145/4320 00:49 < 24:01, 2.90 it/s, Epoch 1/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.848304</td>\n",
              "      <td>0.487482</td>\n",
              "      <td>0.479479</td>\n",
              "      <td>0.483447</td>\n",
              "      <td>0.772708</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(eval_dataset=test_dataset)"
      ],
      "metadata": {
        "id": "QWUjvTzflCIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "rfzaxHYTZPKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "6Fntk0Bwsrun"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}